name: Daily Research Data Update
on:
  schedule:
    # Run at 9:00 AM IST (3:30 AM UTC) on weekdays
    - cron: '30 3 * * 1-5'
  workflow_dispatch:  # Allow manual triggering
  push:
    paths:
      - 'data/raw/*.xlsx'  # Trigger when Excel files are updated

jobs:
  process-research-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas openpyxl xlrd
    
    - name: Check for Excel files
      id: check_files
      run: |
        if [ -f "data/raw/Person-1_Aequitas.xlsx" ] && [ -f "data/raw/Person-2_Aequitas.xlsx" ]; then
          echo "files_exist=true" >> $GITHUB_OUTPUT
        else
          echo "files_exist=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Process Excel files
      if: steps.check_files.outputs.files_exist == 'true'
      run: |
        python scripts/data_processor.py \
          --person1 data/raw/Person-1_Aequitas.xlsx \
          --person2 data/raw/Person-2_Aequitas.xlsx \
          --output data/processed
      env:
        PYTHONPATH: ${{ github.workspace }}
    
    - name: Validate processed data
      if: steps.check_files.outputs.files_exist == 'true'
      run: |
        # Check if JSON files were created
        if [ ! -f "data/processed/consolidated_research_data.json" ]; then
          echo "Error: consolidated_research_data.json not found"
          exit 1
        fi
        
        # Validate JSON structure
        python -c "
        import json
        with open('data/processed/consolidated_research_data.json', 'r') as f:
            data = json.load(f)
        
        required_keys = ['metadata', 'consolidated_topics', 'priority_breakdown', 'sector_analysis']
        for key in required_keys:
            if key not in data:
                raise ValueError(f'Missing required key: {key}')
        
        print('JSON validation successful')
        print(f'Topics processed: {len(data[\"consolidated_topics\"])}')
        print(f'Deduplication rate: {data[\"metadata\"][\"deduplication_rate\"]}%')
        "
    
    - name: Generate processing report
      if: steps.check_files.outputs.files_exist == 'true'
      run: |
        cat > processing_report.md << EOF
        # Research Processing Report
        
        **Date:** $(date '+%Y-%m-%d %H:%M:%S UTC')
        **Workflow:** ${{ github.workflow }}
        **Run:** ${{ github.run_number }}
        
        ## Processing Summary
        
        \`\`\`
        $(python -c "
        import json
        with open('data/processed/consolidated_research_data.json', 'r') as f:
            data = json.load(f)
        
        print(f'Total original items: {data[\"metadata\"][\"total_items\"]}')
        print(f'Consolidated items: {data[\"metadata\"][\"consolidated_items\"]}')
        print(f'Deduplication rate: {data[\"metadata\"][\"deduplication_rate\"]}%')
        print(f'Sectors covered: {len(data[\"metadata\"][\"sectors_covered\"])}')
        print()
        print('Priority breakdown:')
        for priority, count in data['priority_breakdown'].items():
            print(f'  {priority.capitalize()}: {count}')
        print()
        print('Top sectors:')
        sector_items = sorted(data['sector_analysis'].items(), key=lambda x: x[1], reverse=True)
        for sector, count in sector_items[:5]:
            print(f'  {sector}: {count}')
        ")
        \`\`\`
        
        ## Files Updated
        - \`data/processed/consolidated_research_data.json\`
        - \`data/processed/research_data.json\`
        
        ## Next Steps
        The dashboard will automatically load the updated data on the next page refresh.
        EOF
    
    - name: Configure Git
      if: steps.check_files.outputs.files_exist == 'true'
      run: |
        git config --global user.name "Aequitas Research Bot"
        git config --global user.email "research-bot@aequitas.com"
    
    - name: Commit and push changes
      if: steps.check_files.outputs.files_exist == 'true'
      run: |
        git add data/processed/
        git add processing_report.md
        
        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "📊 Automated research data update - $(date '+%Y-%m-%d')"
          git push
          echo "Changes committed and pushed successfully"
        fi
    
    - name: Create processing summary
      if: steps.check_files.outputs.files_exist == 'true'
      run: |
        echo "## Processing Complete ✅" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Dashboard URL:** https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        python -c "
        import json
        with open('data/processed/consolidated_research_data.json', 'r') as f:
            data = json.load(f)
        
        print(f'**Topics Processed:** {data[\"metadata\"][\"consolidated_items\"]}')
        print(f'**Deduplication Rate:** {data[\"metadata\"][\"deduplication_rate\"]}%')
        print(f'**Sectors Covered:** {len(data[\"metadata\"][\"sectors_covered\"])}')
        print()
        print('**Priority Distribution:**')
        for priority, count in data['priority_breakdown'].items():
            print(f'- {priority.capitalize()}: {count}')
        " >> $GITHUB_STEP_SUMMARY
    
    - name: Notify on failure
      if: failure() && steps.check_files.outputs.files_exist == 'true'
      run: |
        echo "## Processing Failed ❌" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "The research data processing workflow failed. Please check the logs for details." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Common issues:" >> $GITHUB_STEP_SUMMARY
        echo "- Excel file format changes" >> $GITHUB_STEP_SUMMARY
        echo "- Missing or corrupted source files" >> $GITHUB_STEP_SUMMARY
        echo "- Data validation errors" >> $GITHUB_STEP_SUMMARY
    
    - name: No files notification
      if: steps.check_files.outputs.files_exist == 'false'
      run: |
        echo "## No Excel Files Found ⚠️" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Expected files not found:" >> $GITHUB_STEP_SUMMARY
        echo "- \`data/raw/Person-1_Aequitas.xlsx\`" >> $GITHUB_STEP_SUMMARY
        echo "- \`data/raw/Person-2_Aequitas.xlsx\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Please upload the Excel files to the \`data/raw/\` directory to enable automated processing." >> $GITHUB_STEP_SUMMARY

  # Optional: Deploy to GitHub Pages after successful processing
  deploy-pages:
    needs: process-research-data
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    permissions:
      contents: read
      pages: write
      id-token: write
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        ref: main  # Ensure we get the latest changes
    
    - name: Setup Pages
      uses: actions/configure-pages@v3
    
    - name: Upload artifact
      uses: actions/upload-pages-artifact@v2
      with:
        path: '.'
    
    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v2
    
    - name: Update deployment summary
      run: |
        echo "## Deployment Complete 🚀" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Live Dashboard:** ${{ steps.deployment.outputs.page_url }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "The updated research dashboard is now live and accessible to the team." >> $GITHUB_STEP_SUMMARY